# AI-Sign-Language-Recognition-System

## ğŸ“Œ Project Overview
This project presents an **AI-powered Sign Language Recognition System** designed to bridge the communication gap between hearing-impaired individuals and the general public. The system uses a **camera-based approach** to recognize hand gestures and convert them into **readable text and speech output** in real time.

The solution leverages **computer vision and deep learning techniques** to accurately identify sign language gestures, making communication more accessible and inclusive.

---

## ğŸ¯ Objectives
- To recognize hand gestures representing sign language using a camera
- To convert recognized signs into **text and speech**
- To support **multiple sign language standards** (country-based switching)
- To provide a **real-time and user-friendly interface**
- To build a low-cost, scalable assistive communication system

---

## ğŸ› ï¸ Technologies Used
- **Programming Language:** Python  
- **Computer Vision:** OpenCV  
- **Deep Learning Framework:** TensorFlow / Keras  
- **Machine Learning Model:** CNN-based gesture classification  
- **Audio Output:** Text-to-Speech (TTS)  
- **Hardware:** Camera, Display, Speaker (optional)

---

## âš™ï¸ System Architecture
1. Camera captures live video frames  
2. OpenCV processes frames and extracts hand regions  
3. Pre-trained deep learning model predicts the sign  
4. Predicted output is converted into text  
5. Text is converted into speech and displayed on screen  

---

## ğŸš€ Features
- Real-time sign language recognition
- High accuracy using deep learning models
- Speech output for recognized gestures
- Modular design for adding new sign languages
- Works with standard webcams
- User-friendly and interactive

---

## ğŸ“‚ Project Structure
